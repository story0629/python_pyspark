{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.107:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "class Utils():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # 敘述性統計：平均數 標準差\n",
    "    def getStatValue(self, df, fieldName):\n",
    "        stat = df.select(avg(fieldName), stddev(fieldName)).collect()\n",
    "        return stat[0]\n",
    "\n",
    "class loadDataCSV(Utils):\n",
    "    # 繼承\n",
    "    def __init__(self):\n",
    "        Utils.__init__(self)\n",
    "        #執行和 Utils的 __init__一樣的事情\n",
    "\n",
    "    # 載入 CSV 檔案\n",
    "    def loadData(self, dataFile, schema, hasHeader=True):\n",
    "        df = sqlContext.read.format('com.databricks.spark.csv').options(header=str(hasHeader)).schema(schema).load(dataFile)\n",
    "        return df\n",
    "        # sqlContext.read.format(\"檔案\")為預設的寫法\n",
    "        # options選項 是否有標題\n",
    "        # schema為資料格式\n",
    "        # load為檔案路徑\n",
    "    \n",
    "    # 列印敘述性統計\n",
    "    def printStats(self, df, fields=None):\n",
    "        if fields is None:\n",
    "            df.describe().show()\n",
    "        else:\n",
    "            for field in fields:\n",
    "                df.describe(field).show()\n",
    "\n",
    "# 主程式\n",
    "def main(sampleDir, dataDir):\n",
    "    # 資料欄位名稱\n",
    "    fields = ['city', '100_house', '100_people', '107_house', '107_people']\n",
    "\n",
    "    # 類別初始化\n",
    "    worker = loadDataCSV()\n",
    "\n",
    "    # 指定資料綱要\n",
    "    schema = StructType([\n",
    "                            StructField(fields[0], StringType()),\n",
    "                            StructField(fields[1], FloatType()),\n",
    "                            StructField(fields[2], FloatType()),\n",
    "                            StructField(fields[3], FloatType()),\n",
    "                            StructField(fields[4], FloatType())\n",
    "                         ])\n",
    "\n",
    "    # 載入無欄位定義 CSV 資料，並轉換資料欄位綱要\n",
    "    df = worker.loadData('%s/NewTaipei.csv' % sampleDir, schema)\n",
    "\n",
    "    # 保存資料集至指定目錄下\n",
    "    df.write.mode('overwrite').save('%s/NewTaipei.parquet' % dataDir,\n",
    "                                        format='parquet')\n",
    "\n",
    "\n",
    "# 程式進入點\n",
    "if __name__ == '__main__':\n",
    "    global sc, sqlContext\n",
    "\n",
    "    # 本地資源運算\n",
    "    appName = 'Cup-00'\n",
    "    master = 'local'\n",
    "\n",
    "    #sc = SparkContext(conf=SparkConf().setAppName(appName).setMaster(master))\n",
    "\n",
    "    # 取得資料庫介面\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    # 調用主程式\n",
    "    homeDir = os.environ['HOME']\n",
    "    dirName = 'Data'\n",
    "    sampleDir = '%s/Sample' % homeDir\n",
    "    dataDir = '%s/Data' % homeDir\n",
    "    \n",
    "    taskControl = [True, False]\n",
    "    #taskControl = [False, True]\n",
    "\n",
    "    if taskControl[0]:\n",
    "        main(sampleDir, dataDir)\n",
    "        #先執行轉檔 csv to parquet\n",
    "\n",
    "    if taskControl[1]:\n",
    "        dataFile = \"%s/IBM.parquet\" % dataDir\n",
    "        sql = 'select date, open, high, low, close, volumn,adjclose, open-close as m,high-low as n  from parquet.`%s` limit 5' % dataFile\n",
    "        df = sqlContext.sql(sql).show()\n",
    "        #sql語法 從parquet抓資料 要的欄位為m1 m2 n1 n2 shape 和 自己想新建ㄉㄜ\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
