{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.173:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----------+-----------------+\n",
      "| m1| m2| n1| n2|      shape|         features|\n",
      "+---+---+---+---+-----------+-----------------+\n",
      "|5.1|3.5|1.4|0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
      "|4.9|3.0|1.4|0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
      "|4.7|3.2|1.3|0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|\n",
      "|4.6|3.1|1.5|0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|\n",
      "|5.0|3.6|1.4|0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|\n",
      "|5.4|3.9|1.7|0.4|Iris-setosa|[5.4,3.9,1.7,0.4]|\n",
      "|4.6|3.4|1.4|0.3|Iris-setosa|[4.6,3.4,1.4,0.3]|\n",
      "|5.0|3.4|1.5|0.2|Iris-setosa|[5.0,3.4,1.5,0.2]|\n",
      "|4.4|2.9|1.4|0.2|Iris-setosa|[4.4,2.9,1.4,0.2]|\n",
      "|4.9|3.1|1.5|0.1|Iris-setosa|[4.9,3.1,1.5,0.1]|\n",
      "|5.4|3.7|1.5|0.2|Iris-setosa|[5.4,3.7,1.5,0.2]|\n",
      "|4.8|3.4|1.6|0.2|Iris-setosa|[4.8,3.4,1.6,0.2]|\n",
      "|4.8|3.0|1.4|0.1|Iris-setosa|[4.8,3.0,1.4,0.1]|\n",
      "|4.3|3.0|1.1|0.1|Iris-setosa|[4.3,3.0,1.1,0.1]|\n",
      "|5.8|4.0|1.2|0.2|Iris-setosa|[5.8,4.0,1.2,0.2]|\n",
      "|5.7|4.4|1.5|0.4|Iris-setosa|[5.7,4.4,1.5,0.4]|\n",
      "|5.4|3.9|1.3|0.4|Iris-setosa|[5.4,3.9,1.3,0.4]|\n",
      "|5.1|3.5|1.4|0.3|Iris-setosa|[5.1,3.5,1.4,0.3]|\n",
      "|5.7|3.8|1.7|0.3|Iris-setosa|[5.7,3.8,1.7,0.3]|\n",
      "|5.1|3.8|1.5|0.3|Iris-setosa|[5.1,3.8,1.5,0.3]|\n",
      "+---+---+---+---+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---+---+---+-----------+-----------------+----------+\n",
      "| m1| m2| n1| n2|      shape|         features|prediction|\n",
      "+---+---+---+---+-----------+-----------------+----------+\n",
      "|5.1|3.5|1.4|0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|         2|\n",
      "|4.9|3.0|1.4|0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|         2|\n",
      "|4.7|3.2|1.3|0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|         2|\n",
      "|4.6|3.1|1.5|0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|         2|\n",
      "|5.0|3.6|1.4|0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|         2|\n",
      "|5.4|3.9|1.7|0.4|Iris-setosa|[5.4,3.9,1.7,0.4]|         2|\n",
      "|4.6|3.4|1.4|0.3|Iris-setosa|[4.6,3.4,1.4,0.3]|         2|\n",
      "|5.0|3.4|1.5|0.2|Iris-setosa|[5.0,3.4,1.5,0.2]|         2|\n",
      "|4.4|2.9|1.4|0.2|Iris-setosa|[4.4,2.9,1.4,0.2]|         2|\n",
      "|4.9|3.1|1.5|0.1|Iris-setosa|[4.9,3.1,1.5,0.1]|         2|\n",
      "|5.4|3.7|1.5|0.2|Iris-setosa|[5.4,3.7,1.5,0.2]|         2|\n",
      "|4.8|3.4|1.6|0.2|Iris-setosa|[4.8,3.4,1.6,0.2]|         2|\n",
      "|4.8|3.0|1.4|0.1|Iris-setosa|[4.8,3.0,1.4,0.1]|         2|\n",
      "|4.3|3.0|1.1|0.1|Iris-setosa|[4.3,3.0,1.1,0.1]|         2|\n",
      "|5.8|4.0|1.2|0.2|Iris-setosa|[5.8,4.0,1.2,0.2]|         2|\n",
      "|5.7|4.4|1.5|0.4|Iris-setosa|[5.7,4.4,1.5,0.4]|         2|\n",
      "|5.4|3.9|1.3|0.4|Iris-setosa|[5.4,3.9,1.3,0.4]|         2|\n",
      "|5.1|3.5|1.4|0.3|Iris-setosa|[5.1,3.5,1.4,0.3]|         2|\n",
      "|5.7|3.8|1.7|0.3|Iris-setosa|[5.7,3.8,1.7,0.3]|         2|\n",
      "|5.1|3.8|1.5|0.3|Iris-setosa|[5.1,3.8,1.5,0.3]|         2|\n",
      "+---+---+---+---+-----------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'printStatsDt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-531e6e5040d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# 載入km這個函式\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-531e6e5040d3>\u001b[0m in \u001b[0;36mKM\u001b[0;34m(df, features)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 result.count()))\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mprintStatsDt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# 程式進入點\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'printStatsDt' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "\n",
    "class Utils():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # 敘述性統計：平均數 標準差\n",
    "    def getStatValue(self, df, fieldName):\n",
    "        stat = df.select(avg(fieldName), stddev(fieldName)).collect()\n",
    "        return stat[0]\n",
    "\n",
    "class loadDataCSV(Utils):\n",
    "    # 繼承\n",
    "    def __init__(self):\n",
    "        Utils.__init__(self)\n",
    "\n",
    "    # 載入 CSV 檔案\n",
    "    def loadData(self, dataFile, schema, hasHeader=False):\n",
    "        df = sqlContext.read.format('com.databricks.spark.csv').options(header=str(hasHeader)).schema(schema).load(dataFile)\n",
    "        return df\n",
    "\n",
    "    # 列印敘述性統計\n",
    "    def printStats(self, df, fields=None):\n",
    "        if fields is None:\n",
    "            df.describe().show()\n",
    "        else:\n",
    "            for field in fields:\n",
    "                df.describe(field).show()\n",
    "\n",
    "# 主程式\n",
    "def main(sampleDir, dataDir):\n",
    "    # 資料欄位名稱\n",
    "    fields = ['m1', 'm2', 'n1', 'n2', 'shape']\n",
    "\n",
    "    # 類別初始化\n",
    "    worker = loadDataCSV()\n",
    "\n",
    "    # 指定資料綱要\n",
    "    schema = StructType([\n",
    "                            StructField(fields[0], DoubleType()),\n",
    "                            StructField(fields[1], DoubleType()),\n",
    "                            StructField(fields[2], DoubleType()),\n",
    "                            StructField(fields[3], DoubleType()),\n",
    "                            StructField(fields[4], StringType())\n",
    "                         ])\n",
    "\n",
    "    # 載入無欄位定義 CSV 資料，並轉換資料欄位綱要\n",
    "    df = worker.loadData('%s/iris.data' % sampleDir, schema)\n",
    "\n",
    "    # 保存資料集至指定目錄下\n",
    "    df.write.mode('overwrite').save('%s/iris.parquet' % dataDir,\n",
    "                                        format='parquet')\n",
    "\n",
    "\n",
    "def KM(df, features):\n",
    "    # 組合自變數欄位群，並指明衍生欄位名稱\n",
    "    # 一樣把 m1,m2,n1,n2仔入features 並輸出\n",
    "    features = (VectorAssembler()\n",
    "                    .setInputCols(features)\n",
    "                    .setOutputCol('features'))\n",
    "    new_df = features.transform(df)\n",
    "    # 新的df原本的5欄 加上features\n",
    "    new_df.show()\n",
    "    \n",
    "    # 取得決策樹介面\n",
    "    dt = KMeans(k=3, seed=1)\n",
    "    # 要分群,要隨機選3個點,依據features分群\n",
    "    model = dt.fit(new_df.select(\"features\"))\n",
    "    \n",
    "    transformed = model.transform(new_df)\n",
    "    transformed.show()\n",
    "    \n",
    "    \n",
    "\n",
    "# 程式進入點\n",
    "if __name__ == '__main__':\n",
    "    global sc, sqlContext\n",
    "\n",
    "    # 本地資源運算\n",
    "    appName = 'Cup-00'\n",
    "    master = 'local'\n",
    "\n",
    "    #sc = SparkContext(conf=SparkConf().setAppName(appName).setMaster(master))\n",
    "\n",
    "    # 取得資料庫介面\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    # 調用主程式\n",
    "    homeDir = os.environ['HOME']\n",
    "    dirName = 'Data'\n",
    "    sampleDir = '%s/Sample' % homeDir\n",
    "    dataDir = '%s/Data' % homeDir\n",
    "    \n",
    "    #taskControl = [True, False]\n",
    "    taskControl = [False, True]\n",
    "\n",
    "    #main(sampleDir, dataDir)\n",
    "\n",
    "    dataFile = \"%s/iris.parquet\" % dataDir\n",
    "    sql = 'select * from parquet.`%s` where m1 is not null' % dataFile\n",
    "    # 原本的df->5欄\n",
    "    df = sqlContext.sql(sql)\n",
    "    \n",
    "    # 載入km這個函式\n",
    "    \n",
    "    result = KM(df, df.columns[0:4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
