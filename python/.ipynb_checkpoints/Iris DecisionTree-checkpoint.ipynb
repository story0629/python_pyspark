{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.137:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- m1: double (nullable = true)\n",
      " |-- m2: double (nullable = true)\n",
      " |-- n1: double (nullable = true)\n",
      " |-- n2: double (nullable = true)\n",
      " |-- shape: string (nullable = true)\n",
      " |-- indexedLabel: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n",
      "+------------+----------+-----------------+-------------+\n",
      "|indexedLabel|prediction|         features|  probability|\n",
      "+------------+----------+-----------------+-------------+\n",
      "|         1.0|       2.0|[4.9,2.4,3.3,1.0]|[0.0,0.0,1.0]|\n",
      "|         1.0|       2.0|[5.9,3.2,4.8,1.8]|[0.0,0.0,1.0]|\n",
      "|         2.0|       1.0|[7.2,3.0,5.8,1.6]|[0.0,1.0,0.0]|\n",
      "+------------+----------+-----------------+-------------+\n",
      "\n",
      "準確率=0.919 (3\t37)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "class Utils():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # 敘述性統計：平均數 標準差\n",
    "    def getStatValue(self, df, fieldName):\n",
    "        stat = df.select(avg(fieldName), stddev(fieldName)).collect()\n",
    "        return stat[0]\n",
    "\n",
    "class LoadSavedData(Utils):\n",
    "    # 繼承\n",
    "    def __init__(self):\n",
    "        Utils.__init__(self)\n",
    "\n",
    "    # 載入資料集檔案\n",
    "    def loadData(self, dataFile):\n",
    "        sql = 'SELECT * FROM parquet.`%s`' % dataFile\n",
    "        df = sqlContext.sql(sql)\n",
    "        return df\n",
    "\n",
    "    # 列印敘述性統計\n",
    "    def printStats(self, df, fields=None):\n",
    "        if fields is None:\n",
    "            df.describe().show()\n",
    "        else:\n",
    "            for field in fields:\n",
    "                df.describe(field).show()\n",
    "\n",
    "    # 決策樹\n",
    "    def DT(self, trainingData, testData, labelIndexer, features):\n",
    "        # 組合自變數欄位群，並指明衍生欄位名稱\n",
    "        '''\n",
    "            例如 m1=2 m2=3 n1= 4 n2=5\n",
    "            features會把它組合成\n",
    "            ['2','3','4','5']\n",
    "        '''\n",
    "        features = (VectorAssembler()\n",
    "                        .setInputCols(features)\n",
    "                        .setOutputCol('features'))\n",
    "\n",
    "        # 取得決策樹介面\n",
    "        dt = DecisionTreeClassifier(labelCol='indexedLabel', featuresCol='features')\n",
    "\n",
    "        # 進行決策樹分析\n",
    "        pipeline = Pipeline(stages=[labelIndexer, features, dt])\n",
    "\n",
    "        # 產生決策樹分析模型\n",
    "        model = pipeline.fit(trainingData)\n",
    "\n",
    "        # 推測值\n",
    "        predictions = model.transform(testData)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    # 列印決策樹分析結果\n",
    "    def printStatsDT(self, predictions):\n",
    "        # 篩選分析結果欄位群\n",
    "        result = predictions.select('indexedLabel', 'prediction', 'features', 'probability')\n",
    "\n",
    "        # 篩選預測錯誤資料\n",
    "        resultError = result.where(result.indexedLabel != result.prediction)\n",
    "        resultError.show()\n",
    "\n",
    "        print(u'準確率=%.3f (%d\\t%d)' % (1.000 - resultError.count() / result.count(),\n",
    "                resultError.count(),\n",
    "                result.count()))\n",
    "\n",
    "\n",
    "# 主程式\n",
    "def main(dataDir):\n",
    "    # 類別初始化\n",
    "    worker = LoadSavedData()\n",
    "\n",
    "    # 載入資料集\n",
    "    df = worker.loadData(dataFile='%s/iris.parquet' % dataDir)\n",
    "\n",
    "    # 資料隨機抽樣成二群\n",
    "    # 將資料分成2群，一群為trainingData,另一個為testData,經過測試 7:3比例ok\n",
    "    (trainingData, testData) = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "    # 為類別值建立數值對照表\n",
    "    # 建立對照表, 輸入的欄位為shape，輸出的為indexedLabel\n",
    "    '''\n",
    "    例如\n",
    "    a  1\n",
    "    a  1\n",
    "    b  2\n",
    "    b  2\n",
    "    b  2\n",
    "    c  0\n",
    "    \n",
    "    c最少->c=0,a第二少->a=1,b最多->b=2\n",
    "    012就是indexedLabel\n",
    "    '''\n",
    "    labelIndexer = StringIndexer(inputCol='shape', outputCol='indexedLabel').fit(df)\n",
    "\n",
    "    # 決策樹：指定自變數欄位群\n",
    "    # 載入worker.DT\n",
    "    result = worker.DT(trainingData, testData, labelIndexer, df.columns[0:4])\n",
    "    result.printSchema()\n",
    "\n",
    "    # 列印決策樹分析結果\n",
    "    worker.printStatsDT(result)\n",
    "\n",
    "# 程式進入點\n",
    "if __name__ == '__main__':\n",
    "    global sc, sqlContext\n",
    "\n",
    "    # 本地資源運算\n",
    "    appName = 'Cup-11'\n",
    "    master = 'local'\n",
    "\n",
    "    #sc = SparkContext(conf=SparkConf().setAppName(appName).setMaster(master))\n",
    "\n",
    "    # 取得資料庫介面\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    # 調用主程式\n",
    "    homeDir = os.environ['HOME']\n",
    "    dirName = 'Data'\n",
    "    sampleDir = '%s/Sample' % homeDir\n",
    "    dataDir = '%s/Data' % homeDir\n",
    "\n",
    "    main(dataDir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
