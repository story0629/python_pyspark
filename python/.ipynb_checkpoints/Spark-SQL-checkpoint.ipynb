{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.173:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----------+------------------+----+\n",
      "| m1| n1| m2| n2|      shape|               mn1| mn2|\n",
      "+---+---+---+---+-----------+------------------+----+\n",
      "|4.9|1.5|3.1|0.1|Iris-setosa|7.3500000000000005|31.0|\n",
      "|4.9|1.5|3.1|0.1|Iris-setosa|7.3500000000000005|31.0|\n",
      "|4.9|1.5|3.1|0.1|Iris-setosa|7.3500000000000005|31.0|\n",
      "+---+---+---+---+-----------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "class Utils():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # 敘述性統計：平均數 標準差\n",
    "    def getStatValue(self, df, fieldName):\n",
    "        stat = df.select(avg(fieldName), stddev(fieldName)).collect()\n",
    "        return stat[0]\n",
    "\n",
    "class loadDataCSV(Utils):\n",
    "    # 繼承\n",
    "    def __init__(self):\n",
    "        Utils.__init__(self)\n",
    "        #執行和 Utils的 __init__一樣的事情\n",
    "\n",
    "    # 載入 CSV 檔案\n",
    "    def loadData(self, dataFile, schema, hasHeader=False):\n",
    "        df = sqlContext.read.format('com.databricks.spark.csv').options(header=str(hasHeader)).schema(schema).load(dataFile)\n",
    "        return df\n",
    "        # sqlContext.read.format(\"檔案\")為預設的寫法\n",
    "        # options選項 是否有標題\n",
    "        # schema為資料格式\n",
    "        # load為檔案路徑\n",
    "    \n",
    "    # 列印敘述性統計\n",
    "    def printStats(self, df, fields=None):\n",
    "        if fields is None:\n",
    "            df.describe().show()\n",
    "        else:\n",
    "            for field in fields:\n",
    "                df.describe(field).show()\n",
    "\n",
    "# 主程式\n",
    "def main(sampleDir, dataDir):\n",
    "    # 資料欄位名稱\n",
    "    fields = ['m1', 'm2', 'n1', 'n2', 'shape']\n",
    "\n",
    "    # 類別初始化\n",
    "    worker = loadDataCSV()\n",
    "\n",
    "    # 指定資料綱要\n",
    "    schema = StructType([\n",
    "                            StructField(fields[0], DoubleType()),\n",
    "                            StructField(fields[1], DoubleType()),\n",
    "                            StructField(fields[2], DoubleType()),\n",
    "                            StructField(fields[3], DoubleType()),\n",
    "                            StructField(fields[4], StringType())\n",
    "                         ])\n",
    "\n",
    "    # 載入無欄位定義 CSV 資料，並轉換資料欄位綱要\n",
    "    df = worker.loadData('%s/iris.data' % sampleDir, schema)\n",
    "\n",
    "    # 保存資料集至指定目錄下\n",
    "    df.write.mode('overwrite').save('%s/iris.parquet' % dataDir,\n",
    "                                        format='parquet')\n",
    "\n",
    "def add_mn1(m1, n1):\n",
    "    return m1*n1\n",
    "\n",
    "# 程式進入點\n",
    "if __name__ == '__main__':\n",
    "    global sc, sqlContext\n",
    "\n",
    "    # 本地資源運算\n",
    "    appName = 'Cup-00'\n",
    "    master = 'local'\n",
    "\n",
    "    #sc = SparkContext(conf=SparkConf().setAppName(appName).setMaster(master))\n",
    "\n",
    "    # 取得資料庫介面\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    # 調用主程式\n",
    "    homeDir = os.environ['HOME']\n",
    "    dirName = 'Data'\n",
    "    sampleDir = '%s/Sample' % homeDir\n",
    "    dataDir = '%s/Data' % homeDir\n",
    "    \n",
    "    #taskControl = [True, False]\n",
    "    taskControl = [False, True]\n",
    "\n",
    "    if taskControl[0]:\n",
    "        main(sampleDir, dataDir)\n",
    "        #先執行轉檔 csv to parquet\n",
    "\n",
    "    if taskControl[1]:\n",
    "        dataFile = \"%s/iris.parquet\" % dataDir\n",
    "        sql = 'select m1, n1, m2,n2,shape, m1*n1 as mn1,m2/n2 as mn2 from parquet.`%s` where m1<5 and m2>3 and m2/n2>20 limit 5' % dataFile\n",
    "        df = sqlContext.sql(sql).show()\n",
    "        #sql語法 從parquet抓資料 要的欄位為m1 m2 n1 n2 shape 和 自己想新建ㄉㄜ\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
