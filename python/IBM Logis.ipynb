{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.173:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volumn: double (nullable = true)\n",
      " |-- adjclose: double (nullable = true)\n",
      " |-- result: integer (nullable = true)\n",
      " |-- indexedLabel: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n",
      "+------------+----------+--------------------+--------------------+\n",
      "|indexedLabel|prediction|            features|         probability|\n",
      "+------------+----------+--------------------+--------------------+\n",
      "|         1.0|       0.0|[122.099998,124.2...|[0.66740492051320...|\n",
      "|         1.0|       0.0|[118.779999,119.6...|[0.51230983286720...|\n",
      "|         1.0|       0.0|[147.399994,147.5...|[0.51094041179876...|\n",
      "|         1.0|       0.0|[152.75,153.69000...|[0.51492470177133...|\n",
      "+------------+----------+--------------------+--------------------+\n",
      "\n",
      "準確率=0.950 (4\t80)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "\n",
    "\n",
    "class Utils():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # 敘述性統計：平均數 標準差\n",
    "    def getStatValue(self, df, fieldName):\n",
    "        stat = df.select(avg(fieldName), stddev(fieldName)).collect()\n",
    "        return stat[0]\n",
    "\n",
    "class LoadSavedData(Utils):\n",
    "    # 繼承\n",
    "    def __init__(self):\n",
    "        Utils.__init__(self)\n",
    "\n",
    "    # 載入資料集檔案\n",
    "    def loadData(self, dataFile):\n",
    "        sql = 'SELECT * FROM parquet.`%s`' % dataFile\n",
    "        df = sqlContext.sql(sql)\n",
    "        return df\n",
    "\n",
    "    # 列印敘述性統計\n",
    "    def printStats(self, df, fields=None):\n",
    "        if fields is None:\n",
    "            df.describe().show()\n",
    "        else:\n",
    "            for field in fields:\n",
    "                df.describe(field).show()\n",
    "\n",
    "    # 羅吉斯回歸\n",
    "    def LR(self, trainingData, testData,\n",
    "            labelIndexer, features,\n",
    "           # 自訂變數\n",
    "            maxIteration=100, regessionParam=0.001):\n",
    "        # 組合自變數欄位群，並指明衍生欄位名稱\n",
    "        features = (VectorAssembler()\n",
    "                        .setInputCols(features)\n",
    "                        .setOutputCol('features'))\n",
    "\n",
    "        # 取得羅吉斯回歸介面\n",
    "        lr = LogisticRegression(labelCol='indexedLabel', featuresCol='features',\n",
    "                                    maxIter=maxIteration, regParam=regessionParam)\n",
    "\n",
    "        # 進行羅吉斯回歸分析\n",
    "        pipeline = Pipeline(stages=[labelIndexer, features, lr])\n",
    "\n",
    "        # 產生羅吉斯回歸分析模型\n",
    "        model = pipeline.fit(trainingData)\n",
    "\n",
    "        # 推測值\n",
    "        predictions = model.transform(testData)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    # 列印羅吉斯回歸分析結果\n",
    "    def printStatsLR(self, predictions):\n",
    "        # 篩選分析結果欄位群\n",
    "        result = predictions.select('indexedLabel', 'prediction', 'features', 'probability')\n",
    "\n",
    "        # 篩選預測錯誤資料\n",
    "        resultError = result.where(result.indexedLabel != result.prediction)\n",
    "        resultError.show()\n",
    "\n",
    "        print(u'準確率=%.3f (%d\\t%d)' % (1.000 - resultError.count() / result.count(),\n",
    "                resultError.count(),\n",
    "                result.count()))\n",
    "\n",
    "def m_fun(o,c):\n",
    "    if o>c:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# 主程式\n",
    "def main(dataDir):\n",
    "    # 類別初始化\n",
    "    worker = LoadSavedData()\n",
    "\n",
    "    # 載入資料集\n",
    "    df = worker.loadData(dataFile='%s/IBM.parquet' % dataDir)\n",
    "    \n",
    "    my_m = udf(m_fun, IntegerType())\n",
    "    df = df.withColumn('result', my_m('open', 'close'))\n",
    "    \n",
    "    # 資料隨機抽樣成二群\n",
    "    (trainingData, testData) = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "    # 為類別值建立數值對照表\n",
    "    labelIndexer = StringIndexer(inputCol='result', outputCol='indexedLabel').fit(df)\n",
    "\n",
    "    # 羅吉斯回歸：指定自變數欄位群\n",
    "    result = worker.LR(trainingData, testData, labelIndexer, df.columns[1:5])\n",
    "    result.printSchema()\n",
    "\n",
    "    # 列印羅吉斯回歸分析結果\n",
    "    worker.printStatsLR(result)\n",
    "\n",
    "# 程式進入點\n",
    "if __name__ == '__main__':\n",
    "    global sc, sqlContext\n",
    "\n",
    "    # 本地資源運算\n",
    "    appName = 'Cup-12'\n",
    "    master = 'local'\n",
    "\n",
    "    #sc = SparkContext(conf=SparkConf().setAppName(appName).setMaster(master))\n",
    "\n",
    "    # 取得資料庫介面\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    # 調用主程式\n",
    "    homeDir = os.environ['HOME']\n",
    "    dirName = 'Data'\n",
    "    sampleDir = '%s/Sample' % homeDir\n",
    "    dataDir = '%s/Data' % homeDir\n",
    "\n",
    "    main(dataDir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
